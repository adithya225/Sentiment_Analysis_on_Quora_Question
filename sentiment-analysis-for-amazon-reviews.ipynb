{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis for Amazon Reviews**\n",
    "\n",
    "In this project, we study the correlation between Amazon's product reviews and the rating of the products given by the customers. \n",
    "\n",
    "We use Natural Language Processing(NLP) to predict whether the sentiment of review is positive or average or negative.\n",
    "\n",
    "Both traditional machine learning algorithms including Bernoulli Naive Bayes,  Multinomial Naive Bayesian(MNB), Logistic regression method and deep neural networks using such as Long Short Term Memory (LSTM) is used. \n",
    "\n",
    "We also use gloVe input features with dropout to obtain results.\n",
    "\n",
    "By comparing these results, we get a good understanding of the these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\n",
      "/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\n",
      "/kaggle/input/consumer-reviews-of-amazon-products/1429_1.csv\n",
      "/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/kaggle/input/consumer-reviews-of-amazon-products/1429_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>keys</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>reviews.date</th>\n",
       "      <th>reviews.dateAdded</th>\n",
       "      <th>reviews.dateSeen</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.id</th>\n",
       "      <th>reviews.numHelpful</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>Kindle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>very fast</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>truman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>Beginner tablet for our 9 year old son.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DaveZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-13T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>Good!!!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVqkIhwDv8e3D1O-lebb</td>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>B01AHB9CN2</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Electronics,iPad &amp; Tablets,All Tablets,Fire Ta...</td>\n",
       "      <td>841667104676,amazon/53004484,amazon/b01ahb9cn2...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>2017-01-12T00:00:00.000Z</td>\n",
       "      <td>2017-07-03T23:33:15Z</td>\n",
       "      <td>2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>http://reviews.bestbuy.com/3545/5620406/review...</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>Fantastic Tablet for kids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explore42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               name  \\\n",
       "0  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "1  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "2  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "3  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "4  AVqkIhwDv8e3D1O-lebb  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...   \n",
       "\n",
       "        asins   brand                                         categories  \\\n",
       "0  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "1  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "2  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "3  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "4  B01AHB9CN2  Amazon  Electronics,iPad & Tablets,All Tablets,Fire Ta...   \n",
       "\n",
       "                                                keys manufacturer  \\\n",
       "0  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "1  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "2  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "3  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "4  841667104676,amazon/53004484,amazon/b01ahb9cn2...       Amazon   \n",
       "\n",
       "               reviews.date     reviews.dateAdded  \\\n",
       "0  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "1  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "2  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "3  2017-01-13T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "4  2017-01-12T00:00:00.000Z  2017-07-03T23:33:15Z   \n",
       "\n",
       "                                    reviews.dateSeen  ... reviews.doRecommend  \\\n",
       "0  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
       "1  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
       "2  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
       "3  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
       "4  2017-06-07T09:04:00.000Z,2017-04-30T00:45:00.000Z  ...                True   \n",
       "\n",
       "  reviews.id  reviews.numHelpful  reviews.rating  \\\n",
       "0        NaN                 0.0             5.0   \n",
       "1        NaN                 0.0             5.0   \n",
       "2        NaN                 0.0             5.0   \n",
       "3        NaN                 0.0             4.0   \n",
       "4        NaN                 0.0             5.0   \n",
       "\n",
       "                                  reviews.sourceURLs  \\\n",
       "0  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "1  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "2  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "3  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "4  http://reviews.bestbuy.com/3545/5620406/review...   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  This product so far has not disappointed. My c...   \n",
       "1  great for beginner or experienced person. Boug...   \n",
       "2  Inexpensive tablet for him to use and learn on...   \n",
       "3  I've had my Fire HD 8 two weeks now and I love...   \n",
       "4  I bought this for my grand daughter when she c...   \n",
       "\n",
       "                             reviews.title reviews.userCity  \\\n",
       "0                                   Kindle              NaN   \n",
       "1                                very fast              NaN   \n",
       "2  Beginner tablet for our 9 year old son.              NaN   \n",
       "3                                  Good!!!              NaN   \n",
       "4                Fantastic Tablet for kids              NaN   \n",
       "\n",
       "   reviews.userProvince  reviews.username  \n",
       "0                   NaN           Adapter  \n",
       "1                   NaN            truman  \n",
       "2                   NaN             DaveZ  \n",
       "3                   NaN            Shacks  \n",
       "4                   NaN         explore42  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviews.rating                                       reviews.text\n",
       "0             5.0  This product so far has not disappointed. My c...\n",
       "1             5.0  great for beginner or experienced person. Boug...\n",
       "2             5.0  Inexpensive tablet for him to use and learn on...\n",
       "3             4.0  I've had my Fire HD 8 two weeks now and I love...\n",
       "4             5.0  I bought this for my grand daughter when she c..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['reviews.rating' , 'reviews.text']]\n",
    "data=data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = data['reviews.rating'].value_counts()\n",
    "plt.bar(counts.index, counts.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the imbalance of our dataset, we try to add in more datas to reduce overfitting.\n",
    "\n",
    "We add files such that we include rows with reviews lesser than 4 inorder to balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(\"/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\n",
    "data2 = data2[['reviews.rating' , 'reviews.text']]\n",
    "data2 = data2[data2[\"reviews.rating\"]<=3]\n",
    "\n",
    "data3 = pd.read_csv(\"/kaggle/input/consumer-reviews-of-amazon-products/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\n",
    "data3 = data3[['reviews.rating' , 'reviews.text']]\n",
    "data3 = data3[data3[\"reviews.rating\"]<=3]\n",
    "\n",
    "frames = [data, data2, data3]\n",
    "df = pd.concat(frames)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have 3 classifications.\n",
    "1. BAD (0) for star rating 1 and 2.\n",
    "2. AVERAGE (1) for star rating 3.\n",
    "3. EXCELLENT (2) for star rating 4 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = {1: 0,\n",
    "            2: 0,\n",
    "            3: 1,\n",
    "            4: 2,\n",
    "            5: 2}\n",
    "\n",
    "df[\"sentiment\"] = df[\"reviews.rating\"].map(sentiment)\n",
    "\n",
    "#print(df[df[\"sentiment\"].isnull()])\n",
    "df[\"sentiment\"] = pd.to_numeric(df[\"sentiment\"], errors='coerce')                                    \n",
    "df = df.dropna(subset=[\"sentiment\"])\n",
    "df[\"sentiment\"]  = df[\"sentiment\"] .astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "Applying various NLP techniques - tokenize and remove all the puncuations and uneseccary jargons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    product far disappoint children love use like ...\n",
       "1       great beginn experienc person bought gift love\n",
       "2    inexpens tablet use learn step nabi thrill lea...\n",
       "3    fire hd two week love tablet great valu prime ...\n",
       "4    bought grand daughter come visit set user ente...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"reviews.text\"]=df[\"reviews.text\"].apply(lambda elem: re.sub(\"[^a-zA-Z]\", \" \", str(elem)))\n",
    "df[\"reviews.text\"]=df[\"reviews.text\"].str.lower()\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_descriptions = df[\"reviews.text\"].str.split()\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "words_descriptions = words_descriptions.apply(lambda elem: [word for word in elem if not word in stopword_list])\n",
    "words_descriptions = words_descriptions.apply(lambda elem: [ps.stem(word) for word in elem])\n",
    "\n",
    "df['cleaned'] = words_descriptions.apply(lambda elem: ' '.join(elem))\n",
    "df['cleaned'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer =TfidfVectorizer()\n",
    "text = vectorizer.fit_transform(df['cleaned']).toarray()\n",
    "texts=pd.DataFrame(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"sentiment\"].values\n",
    "X=pd.DataFrame(texts)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, max_iter=1000, random_state=0, solver='liblinear',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 93.46277459328716\n",
      "Test accuracy : 90.88258680095414\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[ 324   45  148]\n",
      " [  56  207  326]\n",
      " [  29   84 6327]]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.63      0.70       517\n",
      "           1       0.62      0.35      0.45       589\n",
      "           2       0.93      0.98      0.96      6440\n",
      "\n",
      "    accuracy                           0.91      7546\n",
      "   macro avg       0.78      0.65      0.70      7546\n",
      "weighted avg       0.90      0.91      0.90      7546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy :', (lr.score(X_train, y_train))*100)\n",
    "print('Test accuracy :', (lr.score(X_test, y_test))*100)\n",
    "      \n",
    "print('\\n CONFUSION MATRIX')\n",
    "print(confusion_matrix(y_test, lr.predict(X_test)))\n",
    "print('\\nCLASSIFICATION REPORT')\n",
    "print(classification_report(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 87.65448460952254\n",
      "Test accuracy : 87.0924993373973\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[ 129    4  384]\n",
      " [  29    7  553]\n",
      " [   2    2 6436]]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.25      0.38       517\n",
      "           1       0.54      0.01      0.02       589\n",
      "           2       0.87      1.00      0.93      6440\n",
      "\n",
      "    accuracy                           0.87      7546\n",
      "   macro avg       0.74      0.42      0.45      7546\n",
      "weighted avg       0.84      0.87      0.82      7546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy :', (nb.score(X_train, y_train))*100)\n",
    "print('Test accuracy :', (nb.score(X_test, y_test))*100)\n",
    "      \n",
    "print('\\n CONFUSION MATRIX')\n",
    "print(confusion_matrix(y_test, nb.predict(X_test)))\n",
    "print('\\nCLASSIFICATION REPORT')\n",
    "print(classification_report(y_test, nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "dt = BernoulliNB()\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 87.93611875020709\n",
      "Test accuracy : 86.46965279618341\n",
      "\n",
      " CONFUSION MATRIX\n",
      "[[ 248   34  235]\n",
      " [  62  136  391]\n",
      " [  99  200 6141]]\n",
      "\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.48      0.54       517\n",
      "           1       0.37      0.23      0.28       589\n",
      "           2       0.91      0.95      0.93      6440\n",
      "\n",
      "    accuracy                           0.86      7546\n",
      "   macro avg       0.63      0.55      0.58      7546\n",
      "weighted avg       0.84      0.86      0.85      7546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train accuracy :', (dt.score(X_train, y_train))*100)\n",
    "print('Test accuracy :', (dt.score(X_test, y_test))*100)\n",
    "      \n",
    "print('\\n CONFUSION MATRIX')\n",
    "print(confusion_matrix(y_test, dt.predict(X_test)))\n",
    "print('\\nCLASSIFICATION REPORT')\n",
    "print(classification_report(y_test, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RNN - Long Short Term Memory(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Embedding, GRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers import GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer is done to vectorize a text corpus, by turning each text into either a sequence of integers or into a vector.\n",
    "And then we split it to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(df[\"reviews.text\"])\n",
    "word_index = token.word_index\n",
    "max_len = 120\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"reviews.text\"], df[\"sentiment\"], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Converting Text to Sequence\n",
    "\n",
    "2.Padding to ensure that all sequences in a list have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9432, 9432, 28295, 28295)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = token.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding = \"post\",truncating = \"post\")\n",
    "\n",
    "X_test = token.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding = \"post\", truncating = \"post\")\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "len(y_test),len(X_test),len(X_train),len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           225696    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 120, 150)          100200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 521,899\n",
      "Trainable params: 521,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)+1\n",
    "embedding_dim = 16\n",
    "optimizer = Adam(lr=0.0001, decay=0.0001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim , input_length=max_len))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(150, return_sequences=False))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1769/1769 [==============================] - 29s 16ms/step - loss: 0.3136 - accuracy: 0.8562\n",
      "Epoch 2/8\n",
      "1769/1769 [==============================] - 29s 17ms/step - loss: 0.3060 - accuracy: 0.8574\n",
      "Epoch 3/8\n",
      "1769/1769 [==============================] - 29s 16ms/step - loss: 0.3080 - accuracy: 0.8577\n",
      "Epoch 4/8\n",
      "1769/1769 [==============================] - 30s 17ms/step - loss: 0.3079 - accuracy: 0.8577\n",
      "Epoch 5/8\n",
      "1769/1769 [==============================] - 29s 16ms/step - loss: 0.3077 - accuracy: 0.8577\n",
      "Epoch 6/8\n",
      "1769/1769 [==============================] - 29s 17ms/step - loss: 0.3078 - accuracy: 0.8577\n",
      "Epoch 7/8\n",
      "1769/1769 [==============================] - 30s 17ms/step - loss: 0.3077 - accuracy: 0.8577\n",
      "Epoch 8/8\n",
      "1769/1769 [==============================] - 29s 17ms/step - loss: 0.3076 - accuracy: 0.8577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa9fe45da10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=16, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885/885 [==============================] - 6s 7ms/step - loss: 0.3074 - accuracy: 0.8577\n",
      "Train accuracy : 85.76780557632446\n",
      "295/295 [==============================] - 2s 8ms/step - loss: 0.3140 - accuracy: 0.8532\n",
      "Test accuracy : 85.31594276428223\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_train, y_train)\n",
    "print( 'Train accuracy :' , result[1]*100)\n",
    "result = model.evaluate(X_test,y_test)\n",
    "print( 'Test accuracy :' , result[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GloVe\n",
    "\n",
    "GloVe, coined from Global Vectors, is a model for distributed word representation. The model is an unsupervised learning algorithm for obtaining vector representations for words. This is achieved by mapping words into a meaningful space where the distance between words is related to semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "f = open('/kaggle/input/glove6b50dtxt/glove.6B.50d.txt',encoding=\"utf8\")\n",
    "embidx = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coeff = np.asarray(val[1:],dtype = 'float')\n",
    "    embidx[word] = coeff\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embidx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14106\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(word_index)\n",
    "embedding_dim = 50\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embidx.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;\n",
    "\n",
    "print(len(embeddings_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14106, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 50)           705300    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 120, 128)          58880     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               19350     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 783,983\n",
      "Trainable params: 78,683\n",
      "Non-trainable params: 705,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedd_layer = Embedding(vocab_size+1, embedding_dim, input_length=max_len, \n",
    "                         weights=[embeddings_matrix], trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedd_layer)\n",
    "model.add(Bidirectional(LSTM(64 , return_sequences = True , dropout = 0.1 , recurrent_dropout = 0.1)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(150,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation=\"softmax\"))\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = Adam(lr = 0.01) , metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56/56 [==============================] - 54s 961ms/step - loss: 0.1309 - accuracy: 0.9227 - val_loss: 0.1616 - val_accuracy: 0.9048\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 54s 972ms/step - loss: 0.1246 - accuracy: 0.9285 - val_loss: 0.1773 - val_accuracy: 0.9071\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 54s 959ms/step - loss: 0.1197 - accuracy: 0.9292 - val_loss: 0.1654 - val_accuracy: 0.9123\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 54s 963ms/step - loss: 0.1121 - accuracy: 0.9352 - val_loss: 0.1693 - val_accuracy: 0.9132\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 54s 959ms/step - loss: 0.1113 - accuracy: 0.9344 - val_loss: 0.1627 - val_accuracy: 0.9098\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 53s 955ms/step - loss: 0.1023 - accuracy: 0.9417 - val_loss: 0.1714 - val_accuracy: 0.9070\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 53s 951ms/step - loss: 0.0982 - accuracy: 0.9424 - val_loss: 0.1730 - val_accuracy: 0.9116\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 53s 955ms/step - loss: 0.0902 - accuracy: 0.9476 - val_loss: 0.1749 - val_accuracy: 0.9101\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 55s 979ms/step - loss: 0.0882 - accuracy: 0.9497 - val_loss: 0.1752 - val_accuracy: 0.9099\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 55s 979ms/step - loss: 0.0848 - accuracy: 0.9514 - val_loss: 0.1799 - val_accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train,y_train,epochs = 10 , batch_size = 512, validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295/295 [==============================] - 19s 65ms/step - loss: 0.1799 - accuracy: 0.9130\n",
      "Test accuracy : 91.29558801651001\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test,y_test)\n",
    "print('Test accuracy :', result[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We extracted the features of our dataset and built several supervised model based on that. \n",
    "These models not only include traditional algorithms such as naive bayes, logistic regression but also deep learning models such as Recurrent Neural Networks and Convolutional Neural networks. \n",
    "\n",
    "From the results, our highest accuracy on the test set is 91.3% when using LSTM with GloVe and dropout. \n",
    "\n",
    "One of the main reason the accuracy is not high enough is because of the data imbalance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
